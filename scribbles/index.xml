<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Scribbles on :)</title><link>https://aybdee.github.io/scribbles/</link><description>Recent content in Scribbles on :)</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Mon, 30 Oct 2023 10:57:12 +0100</lastBuildDate><atom:link href="https://aybdee.github.io/scribbles/index.xml" rel="self" type="application/rss+xml"/><item><title>Talking about GANS</title><link>https://aybdee.github.io/scribbles/gantalking/</link><pubDate>Mon, 30 Oct 2023 10:57:12 +0100</pubDate><guid>https://aybdee.github.io/scribbles/gantalking/</guid><description>think of an intro (something about how you haven&amp;rsquo;t done serious ML in a while and you decided to blitz through some GAN papers - maybe talk about how you want to colour manga using GANS (link to the paper) and how the paper didn&amp;rsquo;t make any sense at first)
Before talking about GANS i think it&amp;rsquo;d be cool to talk about how computer scientists frame the problem of generating (data?</description><content>&lt;p>think of an intro
(something about how you haven&amp;rsquo;t done serious ML in a while and you decided to blitz through some GAN papers - maybe talk about how you want to colour manga using GANS (link to the paper) and how the paper didn&amp;rsquo;t make any sense at first)&lt;/p>
&lt;p>Before talking about GANS i think it&amp;rsquo;d be cool to talk about how computer scientists frame the problem of generating (data?)&lt;/p>
&lt;p>we look at sample data(thats the patterns we want to generate as probability distributions) across some domain&lt;/p>
&lt;p>if you&amp;rsquo;re not familiar with the term&lt;/p>
&lt;p>&amp;ndash; drop wikipedia? definition of probability distributions here&lt;/p>
&lt;p>probability distributions are the way we represent the likelihood that some data sample will occur at a point(in some number space)&lt;/p>
&lt;p>an example with the MNIST dataset(if you&amp;rsquo;ve never heard of it it&amp;rsquo;s a dataset of handwritten digits sized 28x28)&lt;/p>
&lt;p>since each image is 28x28 pixels we&amp;rsquo;ve got 784 (values?) for each image&lt;/p>
&lt;p>say we represent that in a coordinate system with 784 dimensions(this isn&amp;rsquo;t something people really do - if we needed to though we&amp;rsquo;d need to use some sort of dimensionality reduction first)&lt;/p>
&lt;p>we&amp;rsquo;d have 784^(784) possible data points that could occur
but not every data point would be MNIST digits
in fact most of it would be noise&lt;/p>
&lt;p>a probability distribution for MNIST basically gives us a value between 0 and 1 for each data point that represents how likely that point is to be a number, kinda like a description
all I&amp;rsquo;d need to do to (generate)? numbers if i had this distribution
is take the data points with a high probability in the distribution&lt;/p>
&lt;p>so in a way we can think of generating taking points from a probability distribution of the items we&amp;rsquo;re trying to generate (that are not included in the training set that we used to somehow do this)&lt;/p>
&lt;p>ok so we&amp;rsquo;re hopefully up to speed..&lt;/p>
&lt;p>Generative Adversarial Networks work by trying to make a new probability distribution that is as similar as possible to the intrinsic distribution of our training data&lt;/p>
&lt;p>since all the information we have about this &amp;ldquo;intrinsic&amp;rdquo; distribution is its samples(our training data) you can also say GANs try to estimate a probability distribution(think a description for out ) from its observed samples&lt;/p></content></item><item><title>Replicating v16 spotify recommendation system</title><link>https://aybdee.github.io/scribbles/v16spotify/</link><pubDate>Mon, 15 May 2023 10:57:12 +0100</pubDate><guid>https://aybdee.github.io/scribbles/v16spotify/</guid><description>the initial model for candidate selection was WRMF(weight regularized matrix factorization)
from the name its pretty easy to assume what it&amp;rsquo;s supposed to be doing
basically matrix factorization as used in regular collaborative filtering problems
where the userxitem ie preferences/ratings or whatever floats your boat is factorized into user and item embeddings that can be used for subsequent predictions
I think the regularization employed here is just for more efficiently learning (something i think is pretty relevant in this case since we&amp;rsquo;ve got like 20k&amp;hellip;.</description><content>&lt;p>the initial model for candidate selection was WRMF(weight regularized matrix factorization)&lt;br>
from the name its pretty easy to assume what it&amp;rsquo;s supposed to be doing&lt;/p>
&lt;p>basically matrix factorization as used in regular collaborative filtering problems&lt;/p>
&lt;p>where the userxitem ie preferences/ratings or whatever floats your boat is factorized into user and item embeddings that can be used for subsequent predictions&lt;/p>
&lt;p>I think the regularization employed here is just for more efficiently learning (something i think is pretty relevant in this case since we&amp;rsquo;ve got like 20k&amp;hellip;..)
the creators of v16 link to a paper that more or less describes their specific approach to WRMF
(Collaborative Filtering for Immediate Feedback Datasets)&lt;/p>
&lt;p>it starts by distinguishing what immediate feedback recommendation problems are and the unique challenges that come as a result&lt;/p>
&lt;p>(can&amp;rsquo;t really do the whole thing that would take forever)&lt;/p>
&lt;p>for the most part,Implicit feedback problems occur when there aren&amp;rsquo;t any explicit(for lack of better word) metrics by which users rank the items that they interact with (usually in the form of a star rating or a ranking or something like that)
instead what you have is just the data on how the user interacts with items , so stuff like how many times they bought something or clickedon a product)&lt;/p>
&lt;p>generating recommendations for users from implicit feedback data is significantly different
one because there isn&amp;rsquo;t any way of quantifying negative preference with the data
(in a ranking system a 1 star rating means the user probably dislikes the product but there&amp;rsquo;s no such thing as negatively clicking a lick)&lt;/p>
&lt;p>and then there&amp;rsquo;s also the fact that these kinds of data tend to be more noisy because not all interactions might come from the user
(this is something that is largely mitigated with rating systems because most people ranking and starring products are doing it to show their personal preference in some way and would rarely bother ranking when someone else is involved)&lt;/p>
&lt;p>lastly is just the increase in the overall complexity of the model for solving the problem
with a simple numerical rating the problem is just to predict a specific score and
the metrics for model success are pretty clear
something like root mean square could probably suffice&lt;/p>
&lt;p>with Implicit feedback it&amp;rsquo;s a little different
since we&amp;rsquo;re working with user interactions
the model is predicting the likelihood of the user performing a certain interaction
for all possible interactions&lt;/p>
&lt;p>(that means we&amp;rsquo;ll have to consider things like availability of the item(whether the interaction can even happen in the first place),
competition with other items, and repeat feedback(not quite sure what this is))&lt;/p></content></item></channel></rss>